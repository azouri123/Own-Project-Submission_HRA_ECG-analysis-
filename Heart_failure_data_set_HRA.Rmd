---
title: "Heart Failure Risk Prediction"
author: "Hatem RABEH"
date: "2022-11-19"
output:
  pdf_document:
    df_print: kable
    toc: yes
    toc_depth: 3
  html_document: default
subtitle: '***HarvardX Data Science Professional Certificate: Capstone project(2)***'
fontsize: 12pt
header-includes:
- \usepackage[font={footnotesize,it}, labelfont={bf}]{caption}
- \usepackage{graphicx}
- \usepackage{float}
include-before: '`\newpage{}`{=latex}'
urlcolor: blue
---


```{r instaling packages, include=FALSE}
                                           #############################
                                ########## Installing needed package ##########  
                                          ###########################

if(!require(tidyverse))    install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret))        install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggthemes))     install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(ggcorrplot))   install.packages("ggcorrplot", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(magrittr))     install.packages("magrittr", repos = "http://cran.us.r-project.org")
if(!require(rpart))        install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot))   install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
if(!require(neighbr))      install.packages("neighbr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(formattable)) install.packages("formattable", repos = "http://cran.us.r-project.org")
if(!require(patchwork))     install.packages("patchwork", repos = "http://cran.us.r-project.org")
if(!require(VIM)) install.packages("hot.deck", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("rmarkdown", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")

                                         #############################
                               ########## Uploading needed package ##########  
                                      ############################  

library(tidyverse)
library(kableExtra)
library(formattable)
library(recosystem)  
library(knitr)
library(rmarkdown)
library(Metrics)
library(tidyverse)
library(dplyr)
library(caret)
library(ggthemes)
library(ggcorrplot)
library(randomForest)
library(magrittr)
library(rpart)
library(rpart.plot)
library(neighbr)
library(kableExtra)
library(formattable)
library(corrgram) 
library(patchwork) 
library(ggplot2) 
library(patchwork)
library(VIM)
library(e1071)
library(kableExtra)

```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center" )
```
# 1. Scope:

This is a second project created in the process of obtaining the Data Science professional certificate from Harvardx University.

# 2. Introduction:

Heart failure (HF), also known as congestive heart failure (CHF), is a syndrome, a group of signs and symptoms caused by an impairment of the heart's blood pumping function. Symptoms typically include shortness of breath, excessive fatigue, and leg swelling. The shortness of breath may occur with exertion or while lying down and may wake people up during the night. Chest pain, including angina, is not usually caused by heart failure but may occur if the heart failure was caused by a heart attack. The severity of heart failure is measured by the severity of symptoms during exercise. Other conditions that may have symptoms similar to heart failure include obesity, kidney failure, liver disease, anemia, and thyroid disease. (sources: Wikipedia) 
In 2015, heart failure affected about 40 million people globally. Overall, around 2% of adults have heart failure and in those over the age of 65, this increases to 6â€“10%. Above 75 years old, rates are greater than 10%. (sources: Wikipedia)
People with a high risk of heart failure (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidemia, or already established disease) need early detection and management wherein a machine learning model can be of great help

### 2.1. Objective:

The main goal of this project is to explore through machine learning techniques the impact of several variables on the heart failure rate and if is it possible to predict its occurrence.

### 2.2. Dataset presentation: 

The used data is a published data on Kaggle containing 918 observations and 12 variables. 

The follwing is the dataset variables' description :

1.	Age: age of the patient [years]
2.	Sex: sex of the patient [M: Male, F: Female]
3.	ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]
4.	RestingBP: resting blood pressure [mm Hg]
5.	Cholesterol: serum cholesterol [mm/dl]
6.	FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]
7.	RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
8.	MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]
9.	ExerciseAngina: exercise-induced angina [Y: Yes, N: No]
10.	Oldpeak: oldpeak = ST [Numeric value measured in depression]
11.	ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
12.	HeartDisease: output class [1: heart disease, 0: Normal]

### 2.3. Method and steps to be implemented: 

1)	Import and read the dataset
2)	DATA Wrangling
3)	DATA visualization and DATA Analysis  
4)	Creat Models
5)	Conclusion

# 3.	Methods and analysis

### 3.1. Import and read the dataset

 The github repo with the data set "Heart_failure_data_set" is available here: //github.com/azouri123/Own-Project-Submission_HRA_ECG-analysis-/blob/main/heart_failure_data_set.csv
 
 The file "Heart_failure_data_set.csv" provided in the github repo must be included in the working (project) directory for the code below to run
 
```{r Import and read the dataset, message=FALSE, warning=FALSE}

#Importing the data set: 
set.seed(1, sample.kind="Rounding")
heart_failure_dataset <- read.csv("heart_failure_data_set.csv")
```

### 3.2. DATA Wrangling

#### 3.2.1 presenting the dataset: 

The following is a presentation of the data_set: 

```{r presenting the dataset, message=FALSE, warning=FALSE }
head(heart_failure_dataset)
```

The following is a summarize of the Data_set:

```{r summarize of the Data_set, message=FALSE, warning=FALSE}
summary(heart_failure_dataset)
```

#### 3.2.2 Missing data review :

To better understand the data_set we are going to look for missing data :

```{r Missing data review, message=FALSE, warning=FALSE}
any(is.na(heart_failure_dataset))
```

As described above there is no missing data in our data_set.

Let's now factorize the categorical variables so R doesn't process them as character strings or continuous numeric data : 

#### 3.2.3 Harmonizing the variables

```{r Harmonizing the variables, message=FALSE, warning=FALSE}
heart_failure_dataset$Sex <- as.factor(heart_failure_dataset$Sex)
heart_failure_dataset$ChestPainType <- as.factor(heart_failure_dataset$ChestPainType)
heart_failure_dataset$FastingBS <- as.factor(heart_failure_dataset$FastingBS)
heart_failure_dataset$RestingECG <- as.factor(heart_failure_dataset$RestingECG)
heart_failure_dataset$ExerciseAngina <- as.factor(heart_failure_dataset$ExerciseAngina)
heart_failure_dataset$ST_Slope <- as.factor(heart_failure_dataset$ST_Slope)
```

#### 3.2.4 Check for Outlier: 

 Outliers in a data_set could introduce bias and errors in the data analysis.
 Thus, a check for outliers in dataset is an important setp: 
 
```{r Check for Outlier, message=FALSE, warning=FALSE}
p1 <- heart_failure_dataset %>% ggplot(aes(Age)) + geom_density(fill = "Red") + labs(x="",title = "Age") + theme_minimal()
p2 <- heart_failure_dataset %>% ggplot(aes(Sex)) + geom_bar(aes(fill = Sex)) + labs(x="",title = "Sex") + theme_minimal()
p3 <- heart_failure_dataset %>% ggplot(aes(ChestPainType)) + geom_bar(aes(fill = ChestPainType)) + labs(x="",title = "Chest Pain Type") + theme_minimal()
p4 <- heart_failure_dataset %>% ggplot(aes(RestingBP)) + geom_histogram(colour = "black",fill = "purple") + labs(x="",title = "Resting Blood Pressure") + theme_minimal()
p5 <- heart_failure_dataset %>% ggplot(aes(Cholesterol)) + geom_histogram(colour = "black",fill = "yellow") + labs(x="",title = "Cholesterol Level") + theme_minimal()
p6 <- heart_failure_dataset %>% ggplot(aes(FastingBS)) + geom_bar(fill = "maroon4") + labs(x="",title = "Blood Sugar after Fasting over 120") + theme_minimal()
p7 <- heart_failure_dataset %>% ggplot(aes(RestingECG)) + geom_bar(aes(fill = RestingECG)) + labs(x="",title = "Resting ECG Levels") + theme_minimal()
p8 <- heart_failure_dataset %>% ggplot(aes(MaxHR)) + geom_density(fill = "brown") + labs(x="",title = "Max Heart Rate") + theme_minimal()
p9 <- heart_failure_dataset %>% ggplot(aes(ExerciseAngina)) + geom_bar(aes(fill = ExerciseAngina)) + labs(x="",title = "Exercise-induced angina") + theme_minimal()
p10 <- heart_failure_dataset %>% ggplot(aes(Oldpeak)) + geom_histogram(color = "black", fill = "turquoise") + labs(x="",title = "Oldpeak") + theme_minimal()
p11 <- heart_failure_dataset %>% ggplot(aes(ST_Slope)) + geom_bar(aes(fill = ST_Slope)) + labs(x="",title = "ST Slope") + theme_minimal()
p12 <- heart_failure_dataset %>% ggplot(aes(HeartDisease)) + geom_bar(fill = "purple") + labs(x="",title = "Heart Disease (Y/N)") + theme_minimal()

(p1 | p2 | p3) 
(p4 | p5 | p6) 
(p7 | p8 | p9) 
(p10 | p11 | p12)

```
 
We conclude, that the restingBp = 0 is an outlier and is for sure an error because if RestingBp = 0 so the patient is dead. Moreover, we can not have a cholesterol level = 0. 
So we are going to use data imputation on this outliers.

We will start with the RestingBP: 

##### 3.2.4.1 Data imputation (RestingBP) 

Before imputating data we need to understand it: 

```{r imputating data (RestingBP), message=FALSE, warning=FALSE}
heart_failure_dataset %>% filter(RestingBP == 0)
```
The Patient have a normal restingECG so definitely there was an error.
Different imputation method exist like Hot-deck, clod-deck, mean substitution or Non-negative matrix factorization.

For this, we gonna use the hot-deck imputation which was invented in the 1950s and consists simply of replacing every missing value with the last observed value in the same variable. 

For that we ganna replace the RextingBp = 0 to RextingBp = NA and then use the hotdeck function.

Hot-deck in its vanilla form may break relations between variables, that is why we will impute within domains (HeartDisease ) 

```{r imputating data within_domain (HeartDisease), message=FALSE, warning=FALSE}
heart_failure_dataset$RestingBP <- replace(heart_failure_dataset$RestingBP,heart_failure_dataset$RestingBP == 0,NA) 
## Verifying that there is no more 0 in the RestingBp  
heart_failure_dataset %>% filter(RestingBP == 0)
## Verifying that there is Na in the new dataset 
any(is.na(heart_failure_dataset))
## using the hotdeck function to replace the Na 
heart_failure_dataset <- hotdeck(heart_failure_dataset,domain_var = "HeartDisease")
##Verifying that we do not have any NA
any(is.na(heart_failure_dataset))
```

##### 3.2.4.2 Data imputation (Cholesterol level): 

The other non comprehensive value deducted from the the graphs is the cholesterol level = 0 
For that we will use the mean substitution, but the problem is, that method may attenuates any correlations involving the variable(s) that are imputed. So before that we will look for the variable who the most predict the cholesterol level, thus we can adjust the mean to it: 


```{r Cholesterol level analysis, message=FALSE, warning=FALSE}
ch1 <- heart_failure_dataset %>% filter(Cholesterol > 0) %>% ggplot(aes(x=Age,y=Cholesterol)) + geom_point() + geom_smooth()
ch2 <- heart_failure_dataset %>% filter(Cholesterol > 0) %>% ggplot(aes(x=Sex,y=Cholesterol)) + geom_boxplot(aes(fill = Sex)) + theme(legend.position='none')
ch3 <- heart_failure_dataset %>% filter(Cholesterol > 0) %>% ggplot(aes(x=ChestPainType,y=Cholesterol)) + geom_boxplot(aes(fill = ChestPainType)) + theme(legend.position='none')
ch4 <- heart_failure_dataset %>% filter(Cholesterol > 0) %>% ggplot(aes(x=FastingBS,y=Cholesterol)) + geom_boxplot(aes(fill = FastingBS)) + theme(legend.position='none')
ch5 <- heart_failure_dataset %>% filter(Cholesterol > 0) %>% ggplot(aes(x=RestingECG,y=Cholesterol)) + geom_boxplot(aes(fill = RestingECG)) + theme(legend.position='none')
ch6 <- heart_failure_dataset %>% filter(Cholesterol > 0) %>% ggplot(aes(x=MaxHR,y=Cholesterol)) + geom_point() + geom_smooth()
ch7 <- heart_failure_dataset %>% filter(Cholesterol > 0) %>% ggplot(aes(x=ExerciseAngina,y=Cholesterol)) + geom_boxplot(aes(fill = ExerciseAngina)) + theme(legend.position='none')
ch8 <- heart_failure_dataset %>% filter(Cholesterol > 0) %>% ggplot(aes(x=ST_Slope,y=Cholesterol)) + geom_boxplot(aes(fill = ST_Slope)) + theme(legend.position='none')
design <- "
AABC
DEFF
GGHH
"

ch1 + ch2 + ch3 + ch4 + ch5 + ch6 + ch7 + ch8 +
  plot_layout(design = design) + plot_annotation(title = "Cholesterol vs other variables")

```

The comparison of different variable against the cholesterol level shows that the sex is the variable that impact the most the cholesterol level.
That is why we will replace the Cholesterol level = 0 with the median cholesterol level adjusted to sex :

```{r holesterol level adjusted to sex, message=FALSE, warning=FALSE}
m.Chol.Median <- heart_failure_dataset %>% filter(Sex == "M", Cholesterol > 0) %>% summarise(median(Cholesterol)) %>% as.numeric
f.Chol.Median <- heart_failure_dataset %>% filter(Sex == "F", Cholesterol > 0) %>% summarise(median(Cholesterol)) %>% as.numeric

heart_failure_dataset$Cholesterol[heart_failure_dataset$Sex == "M" & heart_failure_dataset$Cholesterol == 0] <- m.Chol.Median
heart_failure_dataset$Cholesterol[heart_failure_dataset$Sex == "F" & heart_failure_dataset$Cholesterol == 0] <- f.Chol.Median
#Checking the results: 
heart_failure_dataset %>% ggplot(aes(Cholesterol)) + geom_histogram(colour = "black",fill = "Red") + labs(x="",title = "Cholesterol Level") + theme_minimal()
```
We have now a clean and tidy data_set, We can now compare the HeartDisease variable with all other variable to appraise if there is any correlation: 

### 3.3. DATA Analysis: 

We have now a clean and tidy data_set, We can now compare the HeartDisease variable with all other variable to appraise if there is any correlation: 

```{r data_analysis, message=FALSE, warning=FALSE}
heart_failure_dataset %>% corrgram(lower.panel = panel.pie ,upper.panel = panel.shade)
```
The correlogram indicates that the Oldpeak and the MaxHR has important correlation in determining if the person has high risk or not of heart disease. 

To dive deep in this we goona analyse further the relation between the heart diseases and the other variables: 

```{r deep data_analysis, message=FALSE, warning=FALSE}
hd1 <- heart_failure_dataset %>% ggplot(aes(Age)) + geom_histogram(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd2 <- heart_failure_dataset %>% ggplot(aes(Sex)) + geom_bar(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd3 <- heart_failure_dataset %>% ggplot(aes(ChestPainType)) + geom_bar(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd4 <- heart_failure_dataset %>% ggplot(aes(RestingBP)) + geom_histogram(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd5 <- heart_failure_dataset %>% ggplot(aes(Cholesterol)) + geom_histogram(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd6 <- heart_failure_dataset %>% ggplot(aes(FastingBS)) + geom_bar(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd7 <- heart_failure_dataset %>% ggplot(aes(RestingECG)) + geom_bar(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd8 <- heart_failure_dataset %>% ggplot(aes(MaxHR)) + geom_histogram(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd9 <- heart_failure_dataset %>% ggplot(aes(ExerciseAngina)) + geom_bar(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd10 <- heart_failure_dataset %>% ggplot(aes(Oldpeak)) + geom_histogram(aes(fill = factor(HeartDisease))) + theme(legend.position='none')
hd11 <- heart_failure_dataset %>% ggplot(aes(ST_Slope)) + geom_bar(aes(fill = factor(HeartDisease))) + theme(legend.position='none')

(hd1 + (hd2 + hd3)) 
(hd4 + hd5 + hd6)
(hd7 + hd8 + hd9) 
(hd10 + hd11) +
  plot_annotation(title = "Heart Disease corrolation")
```

After comparing HeartDisease with other variable, we can conlcude that: 

  - the higher the Age the higher 
  - The males have higher risk
  - ChestPainType ASY have higher risk)
  - FastingBS >120 increase the risk)
  - MaxHR the lower induce more risk)
  - ExerciseAngina (Y = high risk)
  - Having a ST_Slope Down or Flat is correlated with high risk
  
  
# 4. Results: 
 
After cleaning then analysis our data_set we can build our Models: 
For this project we will use as recommended advanced methods of machine learning: 

  - Logistic Regression,
  - Random Forest,
  - Support Vector Machine,
  - K-nearest neighbour & Neural Nets
 
 ### 4.1. Splitting train_set and test_set
 
 we will create train set(80%) and HF_testset set (20%)
 
```{r spliting dataset, message=FALSE, warning=FALSE}
ind = createDataPartition(heart_failure_dataset$HeartDisease, times = 1, p = 0.8, list = FALSE)
HF_trainset <- heart_failure_dataset[ind,]
HF_testset <- heart_failure_dataset[-ind,]
```

### 4.2. Logistic Regression Model:
  
In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables.
In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination) (sources : Wikipedia)

```{r Logistic Regression Model, message=FALSE, warning=FALSE}
 log.model <- glm(HeartDisease ~ ., data = HF_trainset, family = binomial(link = 'logit'))
summary(log.model)
# Logistic Regression Model heart_failure_dataseting
glm.prediction <- predict(log.model, newdata = HF_testset, type = "response")
glm.prediction <- ifelse(glm.prediction >= 0.5, 1, 0)
#how accurate it is against our test data
table(HF_testset$HeartDisease, glm.prediction)
sum(HF_testset$HeartDisease==glm.prediction) / nrow(HF_testset)
```
 
### 4.3.Random Forest Model : 
 
Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. 
For classification tasks, the output of the random forest is the class selected by most trees. 
For regression tasks, the mean or average prediction of the individual trees is returned.
Random decision forests correct for decision trees' habit of overfitting to their training set.
Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees.
However, data characteristics can affect their performance (sources : Wikipedia)

 
```{r Random Forest Model, message=FALSE, warning=FALSE}
 rf.model <- randomForest(HeartDisease ~ ., data = HF_trainset)
importance(rf.model)

rf.prediction <- predict(rf.model, newdata = HF_testset)
rf.prediction <- ifelse(rf.prediction >= 0.5, 1, 0)

table(HF_testset$HeartDisease, rf.prediction)
sum(HF_testset$HeartDisease==rf.prediction) / nrow(HF_testset)
 
```


### 4.4. Support Vector Machine (linear and radial kernel): 

In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues. SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. (sources : Wikipedia)

```{r Support Vector Machine,message=FALSE, warning=FALSE}
tune.svm.linear <- tune.svm(HeartDisease ~ ., data = HF_trainset, kernel = "linear" , cost=c(0.001,0.01,0.1), gamma=c(0.001,0.01,0.1))
tune.svm.radial <- tune.svm(HeartDisease ~ ., data = HF_trainset, kernel = "radial" , cost=c(0.01,0.1,0.5,1), gamma=c(0.1,0.5,1))
svm.linear <- svm(HeartDisease ~ ., data = HF_trainset, kernel = "linear", cost = 0.01, gamma = 0.001)
svm.radial <- svm(HeartDisease ~ ., data = HF_trainset, kernel = "radial", cost = 1, gamma = 0.1)

svm.linear.pred <- predict(svm.linear, newdata = HF_testset)
svm.linear.pred <- ifelse(svm.linear.pred >= 0.5, 1, 0)
table(HF_testset$HeartDisease,svm.linear.pred)
sum(HF_testset$HeartDisease == svm.linear.pred)/nrow(HF_testset)

svm.radial.pred <- predict(svm.radial, newdata = HF_testset)
svm.radial.pred <- ifelse(svm.radial.pred >= 0.5, 1, 0)
table(HF_testset$HeartDisease,svm.radial.pred)
sum(HF_testset$HeartDisease == svm.radial.pred)/nrow(HF_testset)
```

The following is the list of results of all 3 models tested:

1) Logistic Regression Model : 0.885
2) Random Forest Model : 0.890
3) Support Vector Machine (radial) Model : 0.852
4) Support Vector Machine (linear) Model : 0.885


# 5. Conclusion:
 
The main goal of this project is to explore variables that may be related to heart failure in order to predict it using several techniques of Machine Learning.

We cleaned and analysed the data_set, and then we used several techniques of Machine Learning to build a model focused on maximizing the true predictions of Heart failure.

The machine-learning models which used to predict the heart failure are: Logistic Regression, Forest Model, Support Vector Machine.

The Forest Model can be seen as the best model.

Many thanks to Rafael Irizarry, the course instructor of HarvardXâ€™s Professional Certificate in Data Science, and to the teaching staff who were always at hand to answer questions and queries raised by students.

This edX series has been extreamly valuable. Irizarry delivered engaging lectures and provided a range of useful coding examples throughout the series.

  
  
